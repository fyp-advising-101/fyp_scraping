import logging
import openai
from azure.storage.blob import BlobServiceClient
from azure.identity import DefaultAzureCredential
from azure.keyvault.secrets import SecretClient
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from chroma_db_manager.ChromaDbManager import ChromaDBManager

# Configure logging as in your files
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    filename="app.log",
    filemode="a"
)

# Configuration values (consistent with your existing files)
db_path = "aub_embeddings"
text_container_name = "web-scraper-output"

# Retrieve secrets from Key Vault
VAULT_URL = "https://advising101vault.vault.azure.net"
credential = DefaultAzureCredential()
client = SecretClient(vault_url=VAULT_URL, credential=credential)
AZURE_BLOB_CONNECTION_STRING = client.get_secret("AZURE-BLOB-CONNECTION-STRING").value
OPENAI_API_KEY = client.get_secret("OPENAI-API-KEY").value

def process_one_file_skip_existing(file_name):
    """
    Download a specific text file from Azure Blob Storage, split its content by paragraphs,
    and for each chunk:
      - If the chunk's entry ID already exists in the Chroma DB collection, skip it.
      - Otherwise, add the chunk using your existing Chroma DB manager logic.
    """
    openai.api_key = OPENAI_API_KEY
    blob_service_client = BlobServiceClient.from_connection_string(AZURE_BLOB_CONNECTION_STRING)
    text_container_client = blob_service_client.get_container_client(text_container_name)
    
    logging.info(f"Starting processing of file: {file_name}")
    try:
        blob_client = text_container_client.get_blob_client(file_name)
        content = blob_client.download_blob().readall().decode('utf-8')
    except Exception as e:
        logging.error(f"Failed to download file {file_name} from blob: {e}")
        return

    # Split content into paragraphs using "\n\n" as the primary separator,
    # with fallbacks to newline, period+space, space, and then an empty string.
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1400,         
        chunk_overlap=350,       
        separators=["\n\n", "\n", ". ", " ", ""],  
    )
    document = Document(page_content=content, metadata={"source": file_name})
    chunks = text_splitter.split_documents([document])
    
    # Initialize Chroma DB manager using your existing logic
    manager = ChromaDBManager(db_path=db_path, openai_api_key=OPENAI_API_KEY)
    collection_name = "aub_embeddings"
    # Get the collection so we can check for existing entry IDs.
    collection = manager.get_or_create_collection(collection_name)
    
    for index, chunk in enumerate(chunks):
        entry_id = f"{file_name}-{index}"
        # Check if the entry_id already exists
        existing = collection.get(ids=[entry_id])
        if entry_id in existing.get('ids', []):
            logging.info(f"Skipping chunk {index} for file {file_name} as it already exists")
        else:
            manager.add_or_update_text_entry(collection_name, entry_id, chunk)
            logging.info(f"Processed chunk {index} for file {file_name}")
    
    logging.info(f"Completed processing of file: {file_name}")

if __name__ == "__main__":
    # Use the file name generated by your scraper, e.g.:
    FILE_NAME = "aub.edu.lb_Registrar_catalogue_Documents_g-catalogue-24-25.pdf.txt"
    process_one_file_skip_existing(FILE_NAME)
